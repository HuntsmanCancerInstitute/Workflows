# 18 April 2024 
# David.Nix@Hci.Utah.Edu
# Huntsman Cancer Institute

# General Resources
maxThreads = 50
halfThreads = 25
thirdThreads = 16
email = "david.nix@hci.utah.edu"
tnRunner= "/uufs/chpc.utah.edu/common/PE/hci-bioinformatics1/TNRunner"
tnRunnerWorkflows = tnRunner + "/Workflows"
avatarWorkflows = "/scratch/general/pe-nfs1/u0028003/Avatar/Auto/Workflows"

# Cloud Resources
patientMolRepo = "s3://hcibioinfo-patient-molecular-repo/Patients/"
molRepoIndex = "s3://hcibioinfo-patient-lists/MolecularRepo/"

# GU Group
guEmail = "david.nix@hci.utah.edu,beverly.chigarira@hci.utah.edu,JongTaek.Kim@aruplab.com"
guBucket = "s3://hcibioinfo-gu-patient-molecular-repo/Patients/"

# HEM Group
hemEmail = "david.nix@hci.utah.edu,Nicola.Camp@hci.utah.edu,Myke.Madsen@hci.utah.edu,Brian.Avery@hci.utah.edu"
hemPath = "/uufs/chpc.utah.edu/common/HIPAA/IRB_00088405-2/AVATAR/SourceData/"

# PHI Resources
# subjectRegistry = "/uufs/chpc.utah.edu/common/PE/hci-bioinformatics1/PHI/Registry/"
#subjectRegistry = "/uufs/chpc.utah.edu/common/HIPAA/u0028003/Scratch/Avatar/Auto/Registry/"
#bkupSubjectRegistry = "/uufs/chpc.utah.edu/common/PE/hci-bioinformatics1/Nix/PHI/RegistryBackupDontUse/"
subjectRegistry = "/scratch/general/pe-nfs1/u0028003/Avatar/Auto/RegistryCopy/Registry/"
bkupSubjectRegistry = "/scratch/general/pe-nfs1/u0028003/Avatar/Auto/RegistryCopy/Backup/"

# Apps, aws cli installed and in the path
java = "java -jar -Djava.io.tmpdir=. -Xmx5G"
useq = "/uufs/chpc.utah.edu/common/HIPAA/u0028003/BioApps/USeq/Apps"
htslib = "/uufs/chpc.utah.edu/common/HIPAA/u0028003/BioApps/HtsLib/1.15/bin"
aster = "/uufs/chpc.utah.edu/common/HIPAA/u0028003/BioApps/Aster"

############# Rules ##############

# Workflow trigger, final Rule, cleanup 
rule FinalCleanup:
    input:
        "Status/UploadGUDatasets_COMPLETE",
        "Status/BackupSubjectRegistry_COMPLETE",
        "Status/CopyHemDatasets_COMPLETE"
    output:
        "Status/ALL_COMPLETE"
    log:
        "Logs/finalCleanup.log"
    shell:
        "touch {output} &> {log}"

# "Status/LinkAsterFilesIntoJobsDirs_COMPLETE"
        # "rm -rf AJobs/ &> {log}; touch {output}"
	# "Status/DownloadFastqFiles_COMPLETE",
        # "Status/SyncMainRepo_COMPLETE",
        # "Status/UploadGUDatasets_COMPLETE",
        # "Status/BackupSubjectRegistry_COMPLETE",
        # "Status/CopyHemDatasets_COMPLETE"

# This downloads a variety of Resource files for processing
# Assumes a recent login session key is available, run 
# ~/BioApps/Aster/rwb.linux.x64 project list
# to trigger the web app credentialling.
rule FetchResourceFiles:
    output:
        "Status/FetchResourceFiles_COMPLETE"
    log:
        "Logs/fetchResourceFiles.log"
    threads:
        maxThreads
    shell:
        "rm -rf Avatar_MolecularData_hg38 ResourceFiles &> {log}; "

        "set +e; "

        "python {aster}/support_scripts/download_project.py"
        "  --project-id project-F66v00Q0q4045Q4Y6PY2Xv7F"
        "  --exec {aster}/rwb.linux.x64"
        "  --include /Avatar_MolecularData_hg38/Manifests_and_QC_Files"
        "  --exclude /Avatar_MolecularData_hg38/Manifests_and_QC_Files/Archive"
        "  --no-dry-run &>> {log}; "

        "mkdir ResourceFiles &>> {log}; "
        "mv Avatar_MolecularData_hg38/Manifests_and_QC_Files/* ResourceFiles/ &>> {log}; "
        "rm -rf Avatar_MolecularData_hg38 &>> {log}; "

        "python {aster}/support_scripts/download_project.py"
        "  --project-id project-F66v00Q0q4045Q4Y6PY2Xv7F --exec {aster}/rwb.linux.x64"
        "  --include '*' | grep FASTq > ResourceFiles/fastqDirs.txt 2>> {log}; "

        "while read -r line;"
        "  do x=$(echo $line | cut -d' ' -f4 | sed 's/[\(\)]//g');"
        "  echo $x &>> {log}; " 
        "python {aster}/support_scripts/download_project.py"
        "  --project-id project-F66v00Q0q4045Q4Y6PY2Xv7F --exec {aster}/rwb.linux.x64"
        "  --include $x | grep q.gz | grep -v .md5 | grep download | cut -d' ' -f6"
        "  >> ResourceFiles/fastqFiles.txt; "
        "done < ResourceFiles/fastqDirs.txt; "
        "set -e; "
        "echo STARTING_AWS_LISTING >> {log}; "
        "aws s3 ls {patientMolRepo} --recursive > ResourceFiles/hci_AWSRepoList.txt 2>> {log};"
        "touch {output}"

# Look for new Avatar datasets to process, uses the PHI to identify the patient in the registry
rule avatarDataWrangler:
    input:
        "Status/FetchResourceFiles_COMPLETE"
    output:
        d = "ResourceFiles/downloadAsterFiles.sh",
        c = "Status/AvatarDataWrangler_COMPLETE"
    log:
        "Logs/avatarDataWrangler.log"
    threads:
        halfThreads
    shell:
        "{java} {useq}/AvatarDataWrangler -r ResourceFiles -j AJobs -a FastqCramDownloads "
        "-s {subjectRegistry} -t ResourceFiles/SMM_PHI &> {log};"
        "touch {output.c} "

# Download Aster fastq and cram files
rule DownloadAsterSequenceFiles:
    input:
        "ResourceFiles/downloadAsterFiles.sh"
    output:
        c = "Status/DownloadAsterSequenceFiles_COMPLETE",
        m = "ResourceFiles/moveAsterFiles.sh",
        f = "ResourceFiles/makeSoftLinks.sh"
    log:
        "Logs/downloadAsterSequenceFiles.log"
    threads:
        21
    shell:
        "mkdir -p FastqCramDownloads; "
        "cat {input} | parallel --will-cite --jobs 20 --halt soon,fail=1 &> {log}; "
        "numFail=$(grep FAIL {log} | wc -l); "
        "if (( $numFail > 0 )); then echo 'ERROR: Azure download failures found. Check Log!' &>> {log}; ls DownloadFailure; fi; "
        "touch {output.c}"

# Download files from the AWS Patient Molecular Repository
rule DownloadAwsSequenceFiles:
    input:
        "Status/DownloadAsterSequenceFiles_COMPLETE"
    output:
        "Status/DownloadAwsSequenceFiles_COMPLETE"
    log:
        "Logs/downloadAwsSequenceFiles.log"
    threads:
        maxThreads
    shell:
        "aws='ResourceFiles/awsFilesToDownload.txt' "
        "if [ -f $aws ]; then "
        "   echo 'Need to run S3Copy!' &> {log}; "
        "else "
        "   echo 'No S3Copy needed.' &> {log}; "
        "   touch {output}; "
        "fi "

# Soft link in the downloaded files to the Jobs Fastq folders.  Some will need to be manually merged.
rule LinkAsterFilesIntoJobsDirs:
    input:
        c = "Status/DownloadAwsSequenceFiles_COMPLETE",
        m = "ResourceFiles/moveAsterFiles",
        f = "ResourceFiles/makeSoftLinks.sh"
    output:
        "Status/LinkAsterFilesIntoJobsDirs_COMPLETE"
    log:
        "Logs/linkAsterFilesIntoJobsDirs.log"
    shell:
        "./{input.m} &> {log}; "
        "./{input.f} &>> {log}; "
        "touch {output}"

# Run TNRunner2, a tool for coordinating the execution of many containerized snakemake workflows to process the data
rule TNRunner2:
    input:
       "Status/LinkAsterFilesIntoJobsDirs_COMPLETE"
    output:
        "Status/TNRunner2_COMPLETE"
    log:
        "Logs/tnRunner2.log"
    threads:
        maxThreads
    shell:
        "{java} {useq}/TNRunner2 -p AJobs "
        "-e {tnRunnerWorkflows}/DnaAlignQC "
        "-t {tnRunnerWorkflows}/RnaAlignQC "
        "-m {tnRunnerWorkflows}/Msi "
        "-a {tnRunnerWorkflows}/Annotator "
        "-q {tnRunnerWorkflows}/IlluminaGermline "
        "-h {tnRunnerWorkflows}/GATKGermline/HaplotypeCalling "
        "-j {tnRunnerWorkflows}/GATKGermline/JointGenotyping "
        "-y {tnRunnerWorkflows}/CopyAnalysis/ "
        "-k {tnRunner}/CopyRatioBkgs/Avatar "
        "-c {tnRunnerWorkflows}/SomaticCaller "
        "-f {tnRunnerWorkflows}/StarFusion "
        "-B {tnRunner}/BamPileups/Avatar/ "
        "-L {tnRunnerWorkflows}/LoH "
        "-b {tnRunnerWorkflows}/SampleConcordance "
        "-x 250 -l -P Mixed,IDTv2 &> {log}; "
        "rm -rf AJobs/IlluminaJointGenotyping_* AJobs/GATKJointGenotyping &>> {log}; "
        "touch {output}" 

# "-f {tnRunnerWorkflows}/StarFusion ", some samples taking > 2 days, dropping

# Copy files needed for GQuery
rule GQuery:
    input:
        "Status/TNRunner2_COMPLETE"
    output:
        "Status/GQuery_COMPLETE"
    log:
        "Logs/gquery.log"
    threads:
        thirdThreads
    shell:
        "rm -rf ForGQuery/; mkdir -p ForGQuery/Data/Hg38/Germline/Avatar/Vcf ForGQuery/Data/Hg38/Germline/Avatar/Bed ForGQuery/Data/Hg38/Somatic/Avatar/Vcf ForGQuery/Data/Hg38/Somatic/Avatar/Bed ForGQuery/Data/Hg38/Somatic/Avatar/Fusion ForGQuery/Data/Hg38/Somatic/Avatar/Cnv/ &> {log}; "
        "for x in $(ls AJobs/*/Avatar/*/SomaticVariantCalls/*_Anno/Vcfs/*.anno.vcf.gz* 2> /dev/null || true); do fileName=$(basename $x); coreId=$(echo $x | cut -d'/' -f2); [ -f $x ] && cp $x ForGQuery/Data/Hg38/Somatic/Avatar/Vcf/$coreId'_'$fileName &>> {log}; done; "
        "for x in $(ls AJobs/*/Avatar/*/SomaticVariantCalls/*_Illumina/Bed/*CoveredRegion.bed.gz* 2> /dev/null || true); do fileName=$(basename $x); coreId=$(echo $x | cut -d'/' -f2); [ -f $x ] && cp $x ForGQuery/Data/Hg38/Somatic/Avatar/Bed/$coreId'_'$fileName &>> {log}; done;  "
        "for x in $(ls AJobs/*/Avatar/*/GermlineVariantCalling/*_GATK_Anno/Vcfs/*.anno.vcf.gz* 2> /dev/null || true); do fileName=$(basename $x); coreId=$(echo $x | cut -d'/' -f2); [ -f $x ] && cp $x ForGQuery/Data/Hg38/Germline/Avatar/Vcf/$coreId'_'$fileName &>> {log}; done; "
        "for x in $(ls AJobs/*/Avatar/*/Alignment/*_NormalDNA/QC/*.PassRC.bed.gz* 2> /dev/null || true); do fileName=$(basename $x); coreId=$(echo $x | cut -d'/' -f2); [ -f $x ] && cp $x ForGQuery/Data/Hg38/Germline/Avatar/Bed/$coreId'_'$fileName &>> {log}; done; "
        "for x in $(ls AJobs/*/Avatar/*/CopyAnalysis/*_GATKCopyRatio/Results/*seg.pass.bed.gz* 2> /dev/null || true); do fileName=$(basename $x); coreId=$(echo $x | cut -d'/' -f2); [ -f $x ] && cp $x ForGQuery/Data/Hg38/Somatic/Avatar/Cnv/$coreId'_'$fileName &>> {log}; done;  "
        "for x in $(ls AJobs/*/Avatar/*/RNAAnalysis/*_STARFusion/*.sf.bed.gz* 2> /dev/null || true); do fileName=$(basename $x); coreId=$(echo $x | cut -d'/' -f2); [ -f $x ] && cp $x ForGQuery/Data/Hg38/Somatic/Avatar/Fusion/$coreId'_'$fileName &>> {log}; done;  "
        "touch {output} "

# Copy files needed for cBioPortal
rule cBioPortal:
    input:
        "Status/TNRunner2_COMPLETE"
    output:
        "Status/CBioPortal_COMPLETE"
    log:
        "Logs/cBioPortal.log"
    threads:
        thirdThreads
    shell:
        "rm -rf ForCBioPortal; mkdir -p ForCBioPortal/Germline/Avatar/Vcf ForCBioPortal/Somatic/Avatar/Vcf ForCBioPortal/Somatic/Avatar/Fusion ForCBioPortal/Somatic/Avatar/Cnv/ &> {log}; "
        "cp ResourceFiles/*ClinicalMolLinkage* ForCBioPortal/ &>> {log}; "
        "for x in $(ls AJobs/*/Avatar/*/SomaticVariantCalls/*_Anno/Vcfs/*.anno.vcf.gz* 2> /dev/null || true); do fileName=$(basename $x); coreId=$(echo $x | cut -d'/' -f2); [ -f $x ] && cp $x ForCBioPortal/Somatic/Avatar/Vcf/$coreId'_'$fileName &>> {log}; done; "
        "for x in $(ls AJobs/*/Avatar/*/GermlineVariantCalling/*_GATK_Anno/Vcfs/*.anno.vcf.gz* 2> /dev/null || true); do fileName=$(basename $x); coreId=$(echo $x | cut -d'/' -f2); [ -f $x ] && cp $x ForCBioPortal/Germline/Avatar/Vcf/$coreId'_'$fileName &>> {log}; done; "
        "for x in $(ls AJobs/*/Avatar/*/CopyAnalysis/*_GATKCopyRatio/Results/*called.anno.seg 2> /dev/null || true); do fileName=$(basename $x); coreId=$(echo $x | cut -d'/' -f2); [ -f $x ] && cp $x ForCBioPortal/Somatic/Avatar/Cnv/$coreId'_'$fileName &>> {log}; done;  "
        "for x in $(ls AJobs/*/Avatar/*/RNAAnalysis/*_STARFusion/*.sf.bed.gz* 2> /dev/null || true); do fileName=$(basename $x); coreId=$(echo $x | cut -d'/' -f2); [ -f $x ] && cp $x ForCBioPortal/Somatic/Avatar/Fusion/$coreId'_'$fileName &>> {log}; done;  "
        "touch {output} "

rule TPM4cBioPortal:
    input:
        "Status/CBioPortal_COMPLETE"
    output:
        "ForCBioPortal/aggregateTPM_ForCBio.txt.gz"
    log:
        "Logs/TPM4cBioPortal.log"
    threads:
        thirdThreads
    shell:
        "rm -rf AggregateTPM &> {log}; mkdir -p AggregateTPM/RSEM AggregateTPM/TPM &>> {log}; cd AggregateTPM; "
        "cp ../*Jobs/*/*/*/Alignment/*TumorRNA/Quantitation/RSEM/*TumorRNA_Hg38.genes.results RSEM/ &>> ../{log}; "
        "files=(RSEM/*) &>> ../{log}; "
        "firstFile=${{files[0]}} &>> ../{log}; "
        "java -jar -Xmx1G ~/USeqApps/PrintSelectColumns -i 0 -f $firstFile -n 1 &>> ../{log}; "
        "mv RSEM/*.xls geneIds.txt &>> ../{log}; "
        "java -jar -Xmx1G ~/USeqApps/PrintSelectColumns -i 5 -f RSEM/ -n 1 &>> ../{log}; "
        "mv RSEM/*xls TPM/ &>> ../{log}; cd TPM/; "
        "for x in *xls; do name=$(echo $x | awk -F'_TumorRNA_Hg38.genes.PSC.xls' '{{print $1}}'); mv $x $name; done ;"
        "ls -1 | tr '\\n' '\\t' > ../aggregateTPM.txt ;"
        "echo >> ../aggregateTPM.txt ;"
        "paste ../geneIds.txt * >> ../aggregateTPM.txt ;"
        "cd ../; "
        "java -jar -Xmx20G ~/USeqApps/NormalizedCountCBioFormater "
        "   -e {tnRunner}/AnnotatorData/TpmVstRNASeqParsing/ens106GeneId2Symbol.txt.gz "
        "   -n aggregateTPM.txt &>> ../{log}; "
        "mv aggregateTPM_ForCBio.txt.gz ../ForCBioPortal/ &>> ../{log}; "
        "cd ../; rm -rf AggregateTPM &>> {log}"  

# For GCs
rule GeneticCounselors:
    input:
        "Status/TNRunner2_COMPLETE"
    output:
        "Status/GeneticCounselors_COMPLETE"
    log:
        "Logs/geneticCounselors.log"
    threads:
        thirdThreads
    shell:
        "rm -rf ForGeneticCounselors/; mkdir -p ForGeneticCounselors/GermlineROI/GatkVcf ForGeneticCounselors/GermlineROI/IlluminaVcf ForGeneticCounselors/GermlineROI/CoCalled/AVP/Vcfs ForGeneticCounselors/GermlineROI/CoCalled/AVP/VcfsWithCalls/ ForGeneticCounselors/GermlineROI/CoCalled/AVP/Cram ForGeneticCounselors/GermlineROI/AllCrams ForGeneticCounselors/GermlineROI/CoCalled/Vcfs/  &> {log}; "
        "for x in $(ls AJobs/*/Avatar/*/GermlineVariantCalling/*_GATK_Anno/Vcfs/ROI/*filt.roi.vcf.gz 2> /dev/null || true); do fileName=$(basename $x); coreId=$(echo $x | cut -d'/' -f2); [ -f $x ] && cp $x ForGeneticCounselors/GermlineROI/GatkVcf/$coreId'_'$fileName &>> {log}; done; "
        "for x in $(ls AJobs/*/Avatar/*/GermlineVariantCalling/*_Illumina_Anno/Vcfs/ROI/*filt.roi.vcf.gz 2> /dev/null || true); do fileName=$(basename $x); coreId=$(echo $x | cut -d'/' -f2); [ -f $x ] && cp $x ForGeneticCounselors/GermlineROI/IlluminaVcf/$coreId'_'$fileName &>> {log}; done; "
        "for x in $(ls AJobs/*/Avatar/*/Alignment/*NormalDNA/Alignment/*cram* 2> /dev/null || true); do fileName=$(basename $x); coreId=$(echo $x | cut -d'/' -f2); [ -f $x ] && ln -s $(realpath $x) ForGeneticCounselors/GermlineROI/AllCrams/$coreId'_'$fileName &>> {log}; done; "
        "echo Consensus &>> {log}; "
        "for x in ForGeneticCounselors/GermlineROI/GatkVcf/*vcf.gz; "
        "   do "
        "   name=`basename $x | awk -F'_GATK_Anno_Hg38.anno.filt.roi.vcf.gz' '{{print $1}}'`; "
        "   java -jar -Xmx10G ~/USeqApps/VCFConsensus -p $x -s ForGeneticCounselors/GermlineROI/IlluminaVcf/$name'_Illumina_Anno_Hg38.anno.filt.roi.vcf.gz' -o ForGeneticCounselors/GermlineROI/CoCalled/Vcfs/$name'_GatkIllum.roi.vcf.gz' -c -u &>> {log} &>> {log}; "
        "done; "
        "echo AnnotatedVcfParser &>> {log}; "
        "java -jar -Xmx10G ~/USeqApps/AnnotatedVcfParser -v ForGeneticCounselors/GermlineROI/CoCalled/Vcfs -s ForGeneticCounselors/GermlineROI/CoCalled/AVP/Vcfs -y ~/TNRunner/GermlineFilteringConfigs/strictGermline_AnnotatedVcfParser.config.txt -C 31Dec2024 -T ~/TNRunner/GermlineFilteringConfigs/bestRefSeqForACMGPlus.txt -U {subjectRegistry}currentRegistry_* &>> {log}; "
        "rm ForGeneticCounselors/GermlineROI/CoCalled/AVP/Vcfs/*_Fail.vcf.gz &>> {log}; "
        "mv ForGeneticCounselors/GermlineROI/CoCalled/AVP/Vcfs/*.xls.gz  ForGeneticCounselors/GermlineROI/CoCalled/AVP/ &>> {log}; "
        "mv ForGeneticCounselors/GermlineROI/CoCalled/AVP/Vcfs/impactedGenes.txt.gz  ForGeneticCounselors/GermlineROI/CoCalled/AVP/ &>> {log}; "
        "java -jar -Xmx1G ~/USeqApps/VCF2Bed -v ForGeneticCounselors/GermlineROI/CoCalled/AVP/Vcfs/ -p 500 -s ForGeneticCounselors/GermlineROI/CoCalled/AVP/Bed/ &>> {log}; "
        "java -jar -Xmx10G ~/USeqApps/BedTabix -v ForGeneticCounselors/GermlineROI/CoCalled/AVP/Bed/  -t ~/BioApps/HtsLib/1.15/bin/ &>> {log}; "
        "echo SamtoolsView &>> {log}; "
        "module load samtools/1.16 &>> {log}; "
        "for x in ForGeneticCounselors/GermlineROI/CoCalled/AVP/Bed/*bed.gz; "
        "   do "
        "   name=$(echo $x | awk -F'_GatkIllum.roi_PassPad500bp.bed.gz' '{{print $1}}'); "
        "   name=$(basename $name); "
        "   samtools view --region-file $x -M "
        "   -T ~/TNRunner/GATKResourceBundleAug2021/Homo_sapiens_assembly38.fasta "
        "   -@ 10 --write-index "
        "   -o ForGeneticCounselors/GermlineROI/CoCalled/AVP/Cram/$name.cram "
        "   ForGeneticCounselors/GermlineROI/AllCrams/$name'_NormalDNA_Hg38.cram' &>> {log}; "
        "   mv ForGeneticCounselors/GermlineROI/CoCalled/AVP/Vcfs/$name'_'*'.vcf.gz' ForGeneticCounselors/GermlineROI/CoCalled/AVP/VcfsWithCalls/$name'.vcf.gz' &>> {log}; "
        "done; "
        "echo CleaupOrg &>> {log}; "
        "rm -rf ForGeneticCounselors/GermlineROI/CoCalled/AVP/Vcfs/ &>> {log}; "
        "mv ForGeneticCounselors/GermlineROI/CoCalled/AVP/VcfsWithCalls ForGeneticCounselors/GermlineROI/CoCalled/AVP/Vcfs &>> {log}; "
        "java -jar -Xmx10G ~/USeqApps/VCFTabix -v ForGeneticCounselors/GermlineROI/CoCalled/AVP/Vcfs  -t ~/BioApps/HtsLib/1.15/bin/ &>> {log}; "
        "mv ForGeneticCounselors/GermlineROI/CoCalled/AVP/* ForGeneticCounselors/GermlineROI/ &>> {log}; "
        "rm -rf ForGeneticCounselors/GermlineROI/CoCalled ForGeneticCounselors/GermlineROI/AllCrams ForGeneticCounselors/GermlineROI/GatkVcf/ ForGeneticCounselors/GermlineROI/IlluminaVcf/  &>> {log}; "
        "touch {output} &>> {log}; "

# Generate aggregate QC data
rule AggregateQC:
    input:
        "Status/TNRunner2_COMPLETE"
    output:
        "Status/AggregateQC_COMPLETE"
    log:
        "Logs/aggregateQC.log"
    shell:
        "{java} {useq}/AggregateQCStats2 -s AggregateQC/ -j AJobs/ &> {log}; "
        "touch {output}"

# Clean up the job directories
rule JobCleaner:
    input:
        "Status/CBioPortal_COMPLETE",
        "Status/GQuery_COMPLETE",
        "Status/GeneticCounselors_COMPLETE",
        "Status/AggregateQC_COMPLETE",
        "ForCBioPortal/aggregateTPM_ForCBio.txt.gz"
    output:
        "Status/JobCleaner_COMPLETE"
    log:
        "Logs/jobCleaner.log"
    threads:
        halfThreads
    shell:
        "{java} {useq}/JobCleaner -r AJobs/ -e '.tbi,.crai,.bai,COMPLETE' -n 'Logs,RunScripts' &> {log}; "
        "touch {output}"

# Sync cleaned analysis with main mol bio repo
rule SyncMainRepo:
    input:
         "Status/JobCleaner_COMPLETE"
    output:
        "Status/SyncMainRepo_COMPLETE"
    log:
        "Logs/syncMainRepo.log"
    threads:
        thirdThreads
    shell:
        "echo STARTING &> {log};"
        "aws s3 sync --only-show-errors --follow-symlinks AJobs/ {patientMolRepo} &>> {log}; "
        "touch {output}; echo COMPLETE &>> {log};"

rule UploadGUDatasets:
    input:
         "Status/JobCleaner_COMPLETE"
    output:
        "Status/UploadGUDatasets_COMPLETE"
    log:
        "Logs/uploadGUDatasets.log"
    threads:
        thirdThreads
    shell:
        "echo STARTING &> {log}; rm -rf ForGUGroup; mkdir ForGUGroup; cd ForGUGroup;"
        "for x in $(ls ../AJobs/*/Avatar/*/ClinicalReport/*_GU_*.json 2> /dev/null || true); "
        "do coreId=$(echo $x | cut -d'/' -f3); [ ! -L $coreId ] && ln -s ../AJobs/$coreId .; done;"
        "cd ../;"
        "d=$(date +'%m_%d_%Y');"
        "phiLinks=GU_Avatar_$d'_PHI.txt';"
        "head -n 1 {subjectRegistry}/currentRegistry_* > $phiLinks;"
        "for x in $(ls ForGUGroup/); do grep $x {subjectRegistry}/currentRegistry_* >> $phiLinks; done;"
        "n=$(cat $phiLinks | wc -l);"
        "if [[ $n -gt 0 ]]; then aws s3 sync ForGUGroup/ {guBucket} --only-show-errors --follow-symlinks "
        "--profile GU &>> {log}; echo $n' new Avatar patient datasets have been uploaded into {guBucket} "
        "See and SAVE the attached file to associate coreIds with patient identifiers.' | "
        "mailx -r noreply.bioinfo@hci.utah.edu -a $phiLinks -s 'PHI - New Avatar Datasets '$d "
        "{guEmail}; fi; "
        "touch {output}; echo COMPLETE &>> {log}"

# Backup subject registry with PHI
# Hold off on syncing PHI until we have the approved AWS PHI policies in place
# aws s3 sync /uufs/chpc.utah.edu/common/PE/hci-bioinformatics1/PHI/Registry/ \
#     s3://hcibioinfo-patient-molecular-repo/PHI/Registry/ --quiet
rule BackupSubjectRegistry:
    input:
         "Status/SyncMainRepo_COMPLETE"
    output:
        "Status/BackupSubjectRegistry_COMPLETE"
    log:
        "Logs/backupSubjectRegistry.log"
    shell:
        "rsync -rtq {subjectRegistry} {bkupSubjectRegistry} &> {log}; "
        "touch {output}"

rule CopyHemDatasets:
    input:
         "Status/JobCleaner_COMPLETE",
         "Status/AggregateQC_COMPLETE"
    output:
        "Status/CopyHemDatasets_COMPLETE"
    log:
        "Logs/copyHemDatasets.log"
    threads:
        halfThreads
    shell:
        "rm -rf ForHemGroup; mkdir ForHemGroup; cd ForHemGroup;"
        "for x in $(ls ../AJobs/*/Avatar/*/ClinicalReport/*_HEM_*.json 2> /dev/null || true); "
        "do coreId=$(echo $x | cut -d'/' -f3); [ ! -L $coreId ] && ln -s ../AJobs/$coreId .; done;"
        "cd ../;"
        "d=$(date +'%m_%d_%Y');"
        "phiLinks=HEM_Avatar_$d'_PHI.txt';"
        "head -n 1 {subjectRegistry}/currentRegistry_* > $phiLinks;"
        "for x in $(ls ForHemGroup/); do grep $x {subjectRegistry}currentRegistry_* >> $phiLinks; done;"
        "n=$(cat $phiLinks | wc -l);"
        "if [[ $n -gt 1 ]]; then rsync -rtL ForHemGroup/ {hemPath}Patients/ &> {log};"
        "echo $n' new Avatar patient datasets have been uploaded into the HEME repo on Redwood.' | "
        "mailx -r noreply.bioinfo@hci.utah.edu -s 'New HEM Avatar Datasets '$d "
        "{hemEmail}; fi; cp -f $phiLinks {hemPath} &>> {log}; cp -f ResourceFiles/*ClinicalMolLinkage* {hemPath} &>> {log}; "
        "d=$(date +'%m_%d_%Y'); cp -f AggregateQC/qcStats.xls {hemPath}$d'_qcStats.xlsx' &>> {log}; "
        "touch {output}"
