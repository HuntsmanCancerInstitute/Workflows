# 8 Nov 2023 
# David.Nix@Hci.Utah.Edu
# Huntsman Cancer Institute

# General Resources
maxThreads = 20
halfThreads = 10
email = "david.nix@hci.utah.edu"
tnRunner= "/uufs/chpc.utah.edu/common/PE/hci-bioinformatics1/TNRunner"
tnRunnerWorkflows = tnRunner + "/Workflows"

# S3 Resources
tempusBucket = "s3://tm-huntsman"
patientMolRepo = "s3://hcibioinfo-patient-molecular-repo/Patients/"
molRepoIndex = "s3://hcibioinfo-patient-lists/MolecularRepo/"

# GU Group
guGroup="GU"
guOp="Neeraj_Agarwal Benjamin_Maughan Manish_Kohli Umang_Swami Sumati_Gupta Jennifer_Lloyd Julia_Batten Tenzin_Phunrab Lindsay_Maxwell Jared_Thorley"
guEmail="david.nix@hci.utah.edu,beverly.chigarira@hci.utah.edu,JongTaek.Kim@aruplab.com"
guBucket="s3://hcibioinfo-gu-patient-molecular-repo/Patients/"

# Melanoma Group
melGroup="MEL"
melOp="Siwen_Hu-lieskovan Jeff_Russell Jeff_S-Russell Jeffery_Russell Joanne_Jeter Kenneth_Grossmann Umang_Swami"
melEmail="david.nix@hci.utah.edu,Siwen.Hu-Lieskovan@hci.utah.edu"
melDir="/uufs/chpc.utah.edu/common/HIPAA/IRB_00138167/TempusClinicalTesting/Auto/Patients"
melPhiDir="/uufs/chpc.utah.edu/common/HIPAA/IRB_00138167/TempusClinicalTesting/Auto/PHI"
melKeyWord="melanoma"

# PHI Resources
subjectRegistry = "/uufs/chpc.utah.edu/common/PE/hci-bioinformatics1/PHI/Registry/"
bkupSubjectRegistry = "/uufs/chpc.utah.edu/common/PE/hci-bioinformatics1/Nix/PHI/RegistryBackupDontUse/"
tempusPHIRepo = "/uufs/chpc.utah.edu/common/PE/hci-bioinformatics1/PHI/Tempus/TempusJsonReports_PHI"

# Apps, aws cli installed and in the path
java = "java -jar -Djava.io.tmpdir=. -Xmx5G"
useq = "/uufs/chpc.utah.edu/common/HIPAA/u0028003/BioApps/USeq/Apps"
htslib = "/uufs/chpc.utah.edu/common/HIPAA/u0028003/BioApps/HtsLib/1.15/bin"


############# Rules ##############

# Workflow trigger, final Rule, cleanup 
rule FinalCleanup:
    input:
        "Status/TempusDataWrangler_COMPLETE",
        "Status/MainRepoIndex_COMPLETE",
        "Status/BackupSubjectRegistry_COMPLETE",
        "Status/UploadGUDatasets_COMPLETE",
        "Status/UploadMelDatasets_COMPLETE"
    output:
        "Status/All_COMPLETE"
    log:
        "Logs/finalCleanup.log"
    shell:
        "touch {output} &> {log}"
## "rm -rf TJobs/ &> {log}; touch {output}"


# Look for new Tempus datasets to process, downloades them, uses the PHI to identify the patient in the registry
# Setting the time delay to 5 days since the RNASeq data posts a day or two after the DNA data
# Can't modify all of the files in the bucket so no deletion or mv possible thus using a file to store processed test IDs to skip
rule TempusDataWrangler:
    output:
        "Status/TempusDataWrangler_COMPLETE"
    log:
        "Logs/tempusDataWrangler.log"
    threads:
        maxThreads
    shell:
        "{java} {useq}/TempusDataWrangler -b {tempusBucket} -j TJobs "
        "-r {tempusPHIRepo} -p tempus -i tempusTestIds2Skip.txt "
        "-s Tmp -t 48 "
        "-c {subjectRegistry} &> {log};"
        "touch {output}"

# Run TNRunner2, a tool for coordinating the execution of many containerized snakemake workflows to process the data
rule TNRunner2:
    input:
       "Status/TempusDataWrangler_COMPLETE"
    output:
        "Status/TNRunner2_COMPLETE"
    log:
        "Logs/tnRunner2.log"
    threads:
        maxThreads
    shell:
        "{java} {useq}/TNRunner2  "
        "-p TJobs  "
        "-e {tnRunnerWorkflows}/DnaAlignQC "
        "-t {tnRunnerWorkflows}/RnaAlignQC "
        "-f {tnRunnerWorkflows}/StarFusion "
        "-m {tnRunnerWorkflows}/Msi "
        "-a {tnRunnerWorkflows}/Annotator "
        "-q {tnRunnerWorkflows}/IlluminaGermline "
        "-h {tnRunnerWorkflows}/GATKGermline/HaplotypeCalling "
        "-j {tnRunnerWorkflows}/GATKGermline/JointGenotyping "
        "-w {tnRunner}/Bam/Hg38Exome/NA12878 "
        "-y {tnRunnerWorkflows}/CopyAnalysis "
        "-k {tnRunner}/CopyRatioBkgs/Tempus "
        "-P XO.V1 "
        "-B {tnRunner}/BamPileups/Tempus "
        "-c {tnRunnerWorkflows}/SomaticCaller "
        "-v {tnRunnerWorkflows}/Tempus/TempusVcf "
        "-b {tnRunnerWorkflows}/SampleConcordance "
        "-x 100 -l &> {log}; "
        "rm -rf TJobs/GATKJointGenotyping TJobs/IlluminaJointGenotyping_* &>> {log}; "
        "touch {output} "

# Copy files needed for GQuery
rule GQuery:
    input:
        "Status/TNRunner2_COMPLETE"
    output:
        "Status/GQuery_COMPLETE"
    log:
        "Logs/gquery.log"
    threads:
        halfThreads
    shell:
        "rm -rf ForGQuery; "
        "mkdir -p  "
        "ForGQuery/Data/Hg38/Somatic/Tempus/Vcf  "
        "ForGQuery/Data/Hg38/Somatic/Tempus/Bed  "
        "ForGQuery/Data/Hg38/Somatic/Tempus/Cnv  "
        "ForGQuery/Data/Hg38/Somatic/Tempus/Fusion  "
        "ForGQuery/Data/Hg38/Germline/Tempus/Vcf  "
        "ForGQuery/Data/Hg38/Germline/Tempus/Bed  "
        "&> {log}; "
        ""
        "for x in $(ls TJobs/*/Tempus/*/SomaticVariantCalls/*_Anno/Vcfs/*.anno.vcf.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForGQuery/Data/Hg38/Somatic/Tempus/Vcf/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls TJobs/*/Tempus/*/SomaticVariantCalls/*_Illumina/Bed/*CoveredRegion.bed.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForGQuery/Data/Hg38/Somatic/Tempus/Bed/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls TJobs/*/Tempus/*/RNAAnalysis/*_STARFusion/*sf.bed.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForGQuery/Data/Hg38/Somatic/Tempus/Fusion/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls  TJobs/*/Tempus/*/CopyAnalysis/*CopyRatio/Results/*seg.pass.bed.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForGQuery/Data/Hg38/Somatic/Tempus/Cnv/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls  TJobs/*/Tempus/*/GermlineVariantCalling/*_GATK_Anno/Vcfs/*_Anno_Hg38.anno.vcf.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForGQuery/Data/Hg38/Germline/Tempus/Vcf/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls  TJobs/*/Tempus/*/Alignment/*NormalDNA/QC/*bed.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForGQuery/Data/Hg38/Germline/Tempus/Bed/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "touch {output} "
 

# Copy files needed for GQuery
rule cBioPortal:
    input:
        "Status/TNRunner2_COMPLETE"
    output:
        "Status/CBioPortal_COMPLETE"
    log:
        "Logs/cBioPortal.log"
    threads:
        halfThreads
    shell:
        "rm -rf ForCBioPortal; "
        "mkdir -p  "
        "ForCBioPortal/Data/Hg38/Somatic/Tempus/Vcf  "
        "ForCBioPortal/Data/Hg38/Somatic/Tempus/Cnv  "
        "ForCBioPortal/Data/Hg38/Somatic/Tempus/Fusion  "
        "ForCBioPortal/Data/Hg38/Germline/Tempus/Vcf  "
        "ForCBioPortal/ClinicalReports "
        "&> {log}; "
        ""
        "for x in $(ls TJobs/*/Tempus/*/SomaticVariantCalls/*_Anno/Vcfs/*.anno.vcf.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForCBioPortal/Data/Hg38/Somatic/Tempus/Vcf/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls TJobs/*/Tempus/*/RNAAnalysis/*_STARFusion/*sf.bed.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForCBioPortal/Data/Hg38/Somatic/Tempus/Fusion/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls  TJobs/*/Tempus/*/CopyAnalysis/*CopyRatio/Results/*seg.pass.bed.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForCBioPortal/Data/Hg38/Somatic/Tempus/Cnv/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls  TJobs/*/Tempus/*/GermlineVariantCalling/*_GATK_Anno/Vcfs/*_Anno_Hg38.anno.vcf.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForCBioPortal/Data/Hg38/Germline/Tempus/Vcf/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls TJobs/*/Tempus/*/ClinicalReport/*json 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForCBioPortal/ClinicalReports/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "{java} {useq}/TempusJson2Vcf -j ForCBioPortal/ClinicalReports -s ForCBioPortal/ClinicalReports/ParsedReports/ "
        "-f {tnRunner}/Indexes/B37/human_g1k_v37_decoy_phiXAdaptr.fasta "
        "-b {tnRunner}/Bed/Tempus/gencode.v19.annotation.genes.bed.gz &>> {log}; "
        "mv ForCBioPortal/ClinicalReports/ParsedReports/aggregatePatientInfo.xls ForCBioPortal/ &>> {log}; "
        "rm -rf ForCBioPortal/ClinicalReports/ &>> {log}; "
        "touch {output} "

# Clean up the job directories
rule JobCleaner:
    input:
        "Status/CBioPortal_COMPLETE",
        "Status/GQuery_COMPLETE"
    output:
        "Status/JobCleaner_COMPLETE"
    log:
        "Logs/jobCleaner.log"
    threads:
        maxThreads
    shell:
        "{java} {useq}/JobCleaner -r TJobs/ -e '.tbi,.crai,.bai,COMPLETE,.tar.gz.unpacked' -n 'Logs,RunScripts' &> {log}; "
        "touch {output}"

# Sync cleaned analysis with main mol bio repo
rule SyncMainRepo:
    input:
         "Status/JobCleaner_COMPLETE"
    output:
        "Status/SyncMainRepo_COMPLETE"
    log:
        "Logs/syncMainRepo.log"
    threads:
        maxThreads
    shell:
        "aws s3 sync TJobs/ {patientMolRepo} --no-progress --quiet &> {log}; "
        "touch {output}"

# Sync xml reports with PHI
# Hold off on syncing PHI until we have the approved AWS PHI policies in place
# aws s3 sync tempusDataWrangler/tempusXmlReports_PHI/ s3://hcibioinfo-patient-molecular-repo/PHI/Reports/tempus/ --quiet
#rule SyncPHIReports:
#    input:
#         "Status/SyncMainRepo_COMPLETE"
#    output:
#        "Status/SyncPHIReports_COMPLETE"
#    log:
#        "Logs/syncPHIReports.log"
#    threads:
#        halfThreads
#    shell:
#        "rsync -rtq tempusDataWrangler/tempusXmlReports_PHI/ {tempusPHIRepo}/ &> {log}; "
#        "touch {output}"
        
# Backup subject registry with PHI
# Hold off on syncing PHI until we have the approved AWS PHI policies in place
# aws s3 sync /uufs/chpc.utah.edu/common/PE/hci-bioinformatics1/PHI/Registry/ \
#     s3://hcibioinfo-patient-molecular-repo/PHI/Registry/ --quiet
rule BackupSubjectRegistry:
    input:
         "Status/TempusDataWrangler_COMPLETE"
    output:
        "Status/BackupSubjectRegistry_COMPLETE"
    log:
        "Logs/backupSubjectRegistry.log"
    threads:
        halfThreads
    shell:
        "rsync -rtq {subjectRegistry} {bkupSubjectRegistry} &> {log}; "
        "touch {output}"
        
        
# Create and upload a file catalog into a separate bucket for browsing, there's no PHI in this file list.
rule MainRepoIndex:
    input:
         "Status/SyncMainRepo_COMPLETE"
    output:
        "Status/MainRepoIndex_COMPLETE"
    log:
        "Logs/mainRepoIndex.log"
    threads:
        maxThreads
    shell:
        "aws s3 ls --recursive {patientMolRepo} > Logs/patientMolecularRepoIndex.txt 2> {log}; "
        "aws s3 cp Logs/patientMolecularRepoIndex.txt {molRepoIndex} --quiet &>> {log}; "
        "touch {output}"

rule UploadGUDatasets:
    input:
         "Status/SyncMainRepo_COMPLETE"
    output:
        "Status/UploadGUDatasets_COMPLETE"
    log:
        "Logs/uploadGUDatasets.log"
    threads:
        maxThreads
    shell:
        "rm -rf {guGroup}; mkdir {guGroup}; cd {guGroup};"
        "for p in {guOp}; do for x in $(ls ../TJobs/*/Tempus/*/ClinicalReport/*'_deid_'$p'_'* 2> /dev/null || true); "
        "do coreId=$(echo $x | cut -d'/' -f3); [ ! -L $coreId ] && ln -s ../TJobs/$coreId .; done; done;"
        "cd ../;"
        "d=$(date +'%m_%d_%Y');"
        "phiLinks={guGroup}'_tempus_'$d'_PHI.txt';"
        "head -n 1 {subjectRegistry}/currentRegistry_* > $phiLinks;"
        "for x in $(ls {guGroup}/); do grep $x {subjectRegistry}/currentRegistry_* >> $phiLinks; done;"
        "n=$(cat $phiLinks | wc -l);"
        "if [[ $n -gt 1 ]]; then aws s3 sync {guGroup}/ {guBucket} --quiet --no-progress --follow-symlinks "
        "--profile {guGroup}; echo $n' new Tempus patient datasets have been uploaded into {guBucket} "
        "See and SAVE the attached file to associate coreIds with patient identifiers.' | "
        "mailx -r noreply.bioinfo@hci.utah.edu -a $phiLinks -s 'PHI - New Tempus Datasets '$d "
        "{guEmail}; fi; "
        "touch {output}"

rule UploadMelDatasets:
    input:
         "Status/SyncMainRepo_COMPLETE"
    output:
        "Status/UploadMelDatasets_COMPLETE"
    log:
        "Logs/uploadMelDatasets.log"
    threads:
        maxThreads
    shell:
        "echo Starting... > {log}; "
        "rm -rf {melGroup}; mkdir {melGroup}; cd {melGroup};"
        "echo LookingForMelJsonFiles... >> ../{log}; "
        "for p in {melOp}; "
        "do for x in $(realpath ../TJobs/*/Tempus/*/ClinicalReport/*'_deid_'$p'_'* 2> /dev/null || true); "
        "do ln -s $x .; "
        "done; "
        "done; "
        "echo RemovingNonMelanomaDatasets... >> ../{log}; "
        "for x in *json; "
        "do grep '\"diagnosis\":' $x | grep -i Melanoma > delmeDiagnosis.txt || true; "
        "count=$(cat delmeDiagnosis.txt | wc -l); "
        "if [[ $count -eq 0 ]]; "
        "then rm $x; "
        "fi; "
        "done; "
        "rm -f delmeDiagnosis.txt; "
        "echo LinkingInJobDirs... >> ../{log}; "
        "for x in *json; "
        "do coreId=$(realpath $x | cut -d'/' -f8); "
        "[ ! -L $coreId ] && ln -s ../TJobs/$coreId .; "
        "rm $x; "
        "done; "
        "echo CopyingPHIToFileHeader... >> ../{log}; "
        "cd ../; "
        "d=$(date +'%m_%d_%Y'); "
        "phiLinks={melGroup}'_Tempus_'$d'_PHI.txt'; "
        "head -n 1 {subjectRegistry}currentRegistry_* > $phiLinks; "
        "echo CopyingPHIToFile... >> {log}; "
        "for x in $(ls {melGroup}/); "
        "do grep $x {subjectRegistry}currentRegistry_* >> $phiLinks; "
        "done; "
        "n=$(cat $phiLinks | wc -l); "
        "echo 'RsyncingOver '$n' dirs' >> {log}; "
        "if [[ $n -gt 1 ]]; "
        "then rsync -rtL {melGroup}/ {melDir}/ &>> {log}; "
        "echo Emailing new results to {melEmail}... >> {log}; "
        "echo $n' New Tempus patient datasets have been uploaded into {melDir}/ See and SAVE the attached file to associate coreIds with patient identifiers.' | " 
        "mailx -r noreply.bioinfo@hci.utah.edu -a $phiLinks -s 'PHI - New Tempus Datasets '$d {melEmail} &>> {log} || true; "
        "echo DoneEmailing... >> {log}; "
        "fi; "
        "echo COMPLETE >> {log}; "
        "touch Status/UploadMelDatasets_COMPLETE; "
