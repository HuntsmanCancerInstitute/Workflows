# 18 Aug 2022 
# David.Nix@Hci.Utah.Edu
# Huntsman Cancer Institute

# General Resources
maxThreads = 20
halfThreads = 10
email = "david.nix@hci.utah.edu"
tnRunner= "/uufs/chpc.utah.edu/common/PE/hci-bioinformatics1/TNRunner"
tnRunnerWorkflows = tnRunner + "/Workflows"

# S3 Resources
tempusBucket = "s3://tm-huntsman"
patientMolRepo = "s3://hcibioinfo-patient-molecular-repo/Patients/"
molRepoIndex = "s3://hcibioinfo-patient-lists/MolecularRepo/"

# GU Group
guGroup="GU"
guOp="Neeraj_Agarwal Benjamin_Maughan Manish_Kohli Umang_Swami Sumati_Gupta Jennifer_Lloyd Julia_Batten Tenzin_Phunrab Lindsay_Maxwell Jared_Thorley"
guEmail="david.nix@hci.utah.edu,beverly.chigarira@hci.utah.edu,JongTaek.Kim@aruplab.com"
guBucket="s3://hcibioinfo-gu-patient-molecular-repo/Patients/"

# PHI Resources
subjectRegistry = "/uufs/chpc.utah.edu/common/PE/hci-bioinformatics1/PHI/Registry/"
bkupSubjectRegistry = "/uufs/chpc.utah.edu/common/PE/hci-bioinformatics1/Nix/PHI/RegistryBackupDontUse/"
tempusPHIRepo = "/uufs/chpc.utah.edu/common/PE/hci-bioinformatics1/PHI/Tempus/TempusJsonReports_PHI"
# subjectRegistry = "/scratch/general/pe-nfs1/u0028003/Tempus/SubMatchMaker/Registry/"
# bkupSubjectRegistry = "/scratch/general/pe-nfs1/u0028003/Tempus/SubMatchMaker/Bkup/"



# Apps, aws cli installed and in the path
java = "java -jar -Djava.io.tmpdir=. -Xmx5G"
useq = "/uufs/chpc.utah.edu/common/HIPAA/u0028003/BioApps/USeq/Apps"
htslib = "/uufs/chpc.utah.edu/common/HIPAA/u0028003/BioApps/HtsLib/1.15/bin"


############# Rules ##############

# Workflow trigger, final Rule, cleanup 
rule FinalCleanup:
    input:
        "Status/TempusDataWrangler_COMPLETE",
        "Status/MainRepoIndex_COMPLETE",
        "Status/BackupSubjectRegistry_COMPLETE",
        "Status/UploadGUDatasets_COMPLETE"
    output:
        "Status/All_COMPLETE"
    log:
        "Logs/finalCleanup.log"
    shell:
        "touch {output} &> {log}"
## "rm -rf TJobs/ &> {log}; touch {output}"

# Look for new Tempus datasets to process, downloades them, uses the PHI to identify the patient in the registry
# Setting the time delay to 5 days since the RNASeq data posts a day or two after the DNA data
# Can't modify all of the files in the bucket so no deletion or mv possible thus using a file to store processed test IDs to skip
rule TempusDataWrangler:
    output:
        "Status/TempusDataWrangler_COMPLETE"
    log:
        "Logs/tempusDataWrangler.log"
    threads:
        maxThreads
    shell:
        "{java} {useq}/TempusDataWrangler -b {tempusBucket} -j TJobs "
        "-r {tempusPHIRepo} -p tempus -i tempusTestIds2Skip.txt "
        "-s Tmp -t 48 "
        "-c {subjectRegistry} &> {log};"
        "touch {output}"

# Run TNRunner2, a tool for coordinating the execution of many containerized snakemake workflows to process the data
rule TNRunner2:
    input:
       "Status/TempusDataWrangler_COMPLETE"
    output:
        "Status/TNRunner2_COMPLETE"
    log:
        "Logs/tnRunner2.log"
    threads:
        maxThreads
    shell:
        "{java} {useq}/TNRunner2  "
        "-p TJobs  "
        "-e {tnRunnerWorkflows}/DnaAlignQC "
        "-t {tnRunnerWorkflows}/RnaAlignQC "
        "-f {tnRunnerWorkflows}/StarFusion "
        "-m {tnRunnerWorkflows}/Msi "
        "-a {tnRunnerWorkflows}/Annotator "
        "-q {tnRunnerWorkflows}/IlluminaGermline "
        "-h {tnRunnerWorkflows}/GATKGermline/HaplotypeCalling "
        "-j {tnRunnerWorkflows}/GATKGermline/JointGenotyping "
        "-w {tnRunner}/Bam/Hg38Exome/NA12878 "
        "-y {tnRunnerWorkflows}/CopyAnalysis "
        "-k {tnRunner}/CopyRatioBkgs/Tempus "
        "-P XO.V1 "
        "-B {tnRunner}/BamPileups/Tempus "
        "-c {tnRunnerWorkflows}/SomaticCaller "
        "-v {tnRunnerWorkflows}/Tempus/TempusVcf "
        "-b {tnRunnerWorkflows}/SampleConcordance "
        "-x 100 -l  &> {log}; "
        "touch {output} "

# Copy files needed for GQuery
rule GQuery:
    input:
        "Status/TNRunner2_COMPLETE"
    output:
        "Status/GQuery_COMPLETE"
    log:
        "Logs/gquery.log"
    threads:
        halfThreads
    shell:
        "rm -rf ForGQuery; "
        "mkdir -p  "
        "ForGQuery/Data/Hg38/Somatic/Tempus/Vcf  "
        "ForGQuery/Data/Hg38/Somatic/Tempus/Bed  "
        "ForGQuery/Data/Hg38/Somatic/Tempus/Cnv  "
        "ForGQuery/Data/Hg38/Somatic/Tempus/Fusion  "
        "ForGQuery/Data/Hg38/Germline/Tempus/Vcf  "
        "ForGQuery/Data/Hg38/Germline/Tempus/Bed  "
        "&> {log}; "
        ""
        "for x in $(ls TJobs/*/Tempus/*/SomaticVariantCalls/*_Anno/Vcfs/*.anno.vcf.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForGQuery/Data/Hg38/Somatic/Tempus/Vcf/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls TJobs/*/Tempus/*/SomaticVariantCalls/*_Illumina/Bed/*CoveredRegion.bed.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForGQuery/Data/Hg38/Somatic/Tempus/Bed/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls TJobs/*/Tempus/*/RNAAnalysis/*_STARFusion/*sf.bed.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForGQuery/Data/Hg38/Somatic/Tempus/Fusion/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls  TJobs/*/Tempus/*/CopyAnalysis/*CopyRatio/Results/*seg.pass.bed.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForGQuery/Data/Hg38/Somatic/Tempus/Cnv/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls  TJobs/*/Tempus/*/GermlineVariantCalling/*_GATK_Anno/Vcfs/*_Anno_Hg38.anno.vcf.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForGQuery/Data/Hg38/Germline/Tempus/Vcf/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls  TJobs/*/Tempus/*/Alignment/*NormalDNA/QC/*bed.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForGQuery/Data/Hg38/Germline/Tempus/Bed/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "touch {output} "
 

# Copy files needed for GQuery
rule cBioPortal:
    input:
        "Status/TNRunner2_COMPLETE"
    output:
        "Status/CBioPortal_COMPLETE"
    log:
        "Logs/cBioPortal.log"
    threads:
        halfThreads
    shell:
        "rm -rf ForCBioPortal; "
        "mkdir -p  "
        "ForCBioPortal/Data/Hg38/Somatic/Tempus/Vcf  "
        "ForCBioPortal/Data/Hg38/Somatic/Tempus/Cnv  "
        "ForCBioPortal/Data/Hg38/Somatic/Tempus/Fusion  "
        "ForCBioPortal/Data/Hg38/Germline/Tempus/Vcf  "
        "ForCBioPortal/ClinicalReports "
        "&> {log}; "
        ""
        "for x in $(ls TJobs/*/Tempus/*/SomaticVariantCalls/*_Anno/Vcfs/*.anno.vcf.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForCBioPortal/Data/Hg38/Somatic/Tempus/Vcf/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls TJobs/*/Tempus/*/RNAAnalysis/*_STARFusion/*sf.bed.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForCBioPortal/Data/Hg38/Somatic/Tempus/Fusion/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls  TJobs/*/Tempus/*/CopyAnalysis/*CopyRatio/Results/*seg.pass.bed.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForCBioPortal/Data/Hg38/Somatic/Tempus/Cnv/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls  TJobs/*/Tempus/*/GermlineVariantCalling/*_GATK_Anno/Vcfs/*_Anno_Hg38.anno.vcf.gz* 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForCBioPortal/Data/Hg38/Germline/Tempus/Vcf/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "for x in $(ls TJobs/*/Tempus/*/ClinicalReport/*json 2> /dev/null || true); "
        "do fileName=$(basename $x); "
        "coreId=$(echo $x | cut -d'/' -f2); "
        "[ -f $x ] && cp $x ForCBioPortal/ClinicalReports/$coreId'_'$fileName &>> {log}; "
        "done; "
        ""
        "{java} {useq}/TempusJson2Vcf -j ForCBioPortal/ClinicalReports -s ForCBioPortal/ClinicalReports/ParsedReports/ "
        "-f {tnRunner}/Indexes/B37/human_g1k_v37_decoy_phiXAdaptr.fasta "
        "-b {tnRunner}/Bed/Tempus/gencode.v19.annotation.genes.bed.gz &>> {log}; "
        "mv ForCBioPortal/ClinicalReports/ParsedReports/aggregatePatientInfo.xls ForCBioPortal/ &>> {log}; "
        "rm -rf ForCBioPortal/ClinicalReports/ &>> {log}; "
        "touch {output} "

# Clean up the job directories
rule JobCleaner:
    input:
        "Status/CBioPortal_COMPLETE",
        "Status/GQuery_COMPLETE"
    output:
        "Status/JobCleaner_COMPLETE"
    log:
        "Logs/jobCleaner.log"
    threads:
        maxThreads
    shell:
        "{java} {useq}/JobCleaner -r TJobs/ -e '.tbi,.crai,.bai,COMPLETE,.tar.gz.unpacked' -n 'Logs,RunScripts' &> {log}; "
        "mkdir -p GermlineGroupProcessing &>> {log}; " 
        "mv -f TJobs/GATKJointGenotyping GermlineGroupProcessing/ &>> {log} || true ; "
        "mv -f TJobs/IlluminaJointGenotyping* GermlineGroupProcessing/ &>> {log} || true ; "
        "touch {output}"

# Sync cleaned analysis with main mol bio repo
rule SyncMainRepo:
    input:
         "Status/JobCleaner_COMPLETE"
    output:
        "Status/SyncMainRepo_COMPLETE"
    log:
        "Logs/syncMainRepo.log"
    threads:
        maxThreads
    shell:
        "aws s3 sync TJobs/ {patientMolRepo} --quiet &> {log}; "
        "touch {output}"

# Sync xml reports with PHI
# Hold off on syncing PHI until we have the approved AWS PHI policies in place
# aws s3 sync tempusDataWrangler/tempusXmlReports_PHI/ s3://hcibioinfo-patient-molecular-repo/PHI/Reports/tempus/ --quiet
#rule SyncPHIReports:
#    input:
#         "Status/SyncMainRepo_COMPLETE"
#    output:
#        "Status/SyncPHIReports_COMPLETE"
#    log:
#        "Logs/syncPHIReports.log"
#    threads:
#        halfThreads
#    shell:
#        "rsync -rtq tempusDataWrangler/tempusXmlReports_PHI/ {tempusPHIRepo}/ &> {log}; "
#        "touch {output}"
        
# Backup subject registry with PHI
# Hold off on syncing PHI until we have the approved AWS PHI policies in place
# aws s3 sync /uufs/chpc.utah.edu/common/PE/hci-bioinformatics1/PHI/Registry/ \
#     s3://hcibioinfo-patient-molecular-repo/PHI/Registry/ --quiet
rule BackupSubjectRegistry:
    input:
         "Status/TempusDataWrangler_COMPLETE"
    output:
        "Status/BackupSubjectRegistry_COMPLETE"
    log:
        "Logs/backupSubjectRegistry.log"
    threads:
        halfThreads
    shell:
        "rsync -rtq {subjectRegistry} {bkupSubjectRegistry} &> {log}; "
        "touch {output}"
        
        
# Create and upload a file catalog into a separate bucket for browsing, there's no PHI in this file list.
rule MainRepoIndex:
    input:
         "Status/SyncMainRepo_COMPLETE"
    output:
        "Status/MainRepoIndex_COMPLETE"
    log:
        "Logs/mainRepoIndex.log"
    threads:
        maxThreads
    shell:
        "aws s3 ls --recursive {patientMolRepo} > Logs/patientMolecularRepoIndex.txt 2> {log}; "
        "aws s3 cp Logs/patientMolecularRepoIndex.txt {molRepoIndex} --quiet &>> {log}; "
        "touch {output}"

rule UploadGUDatasets:
    input:
         "Status/SyncMainRepo_COMPLETE"
    output:
        "Status/UploadGUDatasets_COMPLETE"
    log:
        "Logs/uploadGUDatasets.log"
    threads:
        maxThreads
    shell:
        "rm -rf {guGroup}; mkdir {guGroup}; cd {guGroup};"
        "for p in {guOp}; do for x in $(ls ../TJobs/*/Tempus/*/ClinicalReport/*'_deid_'$p'_'* 2> /dev/null || true); "
        "do coreId=$(echo $x | cut -d'/' -f3); [ ! -L $coreId ] && ln -s ../TJobs/$coreId .; done; done;"
        "cd ../;"
        "d=$(date +'%m_%d_%Y');"
        "phiLinks={guGroup}'_tempus_'$d'_PHI.txt';"
        "head -n 1 {subjectRegistry}/currentRegistry_* > $phiLinks;"
        "for x in $(ls {guGroup}/); do grep $x {subjectRegistry}/currentRegistry_* >> $phiLinks; done;"
        "n=$(cat $phiLinks | wc -l);"
        "if [[ $n -gt 1 ]]; then aws s3 sync {guGroup}/ {guBucket} --quiet --follow-symlinks "
        "--profile {guGroup}; echo $n' new tempus patient datasets have been uploaded into {guBucket} "
        "See and SAVE the attached file to associate coreIds with patient identifiers.' | "
        "mailx -r noreply.bioinfo@hci.utah.edu -a $phiLinks -s 'PHI - New Tempus Datasets '$d "
        "{guEmail}; fi; "
        "touch {output}"
